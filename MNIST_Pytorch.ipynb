{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dettmar/machine-learning/blob/master/MNIST_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As3VG_SogQmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DESeSerNKXJ",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/pytorch/examples/blob/master/mnist/main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvFWOOENgxB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms = [\n",
        "  torchvision.transforms.ToTensor(),\n",
        "  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "]\n",
        "augmentations = [\n",
        "  #torchvision.transforms.RandomAffine([-90, 90], translate=[0.2, 0.2], scale=[0.5, 0.5], shear=[-10, 10]),\n",
        "  torchvision.transforms.RandomAffine([-45, 45]),\n",
        "  torchvision.transforms.RandomAffine([-45, 45], scale=[0.9, 1.1]),\n",
        "  torchvision.transforms.RandomAffine([0,0], shear=[-45, 45]),\n",
        "  torchvision.transforms.RandomAffine([-30, 30], scale=[0.9, 1.1], shear=[-45, 45]),\n",
        "  torchvision.transforms.RandomPerspective(distortion_scale=.5, p=1, interpolation=3)\n",
        "]\n",
        "# original untouched\n",
        "traintransforms = torchvision.transforms.Compose(transforms)\n",
        "trainset = datasets.MNIST('./data', train=True, download=True, transform=traintransforms)\n",
        "\n",
        "for augmentation in augmentations:\n",
        "  traintransforms = torchvision.transforms.Compose([augmentation] + transforms)\n",
        "  modifiedtrainset = datasets.MNIST('./data', train=True, download=True, transform=traintransforms)\n",
        "  trainset = torch.utils.data.ConcatDataset([trainset, modifiedtrainset])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=125, shuffle=True)\n",
        "\n",
        "testtransforms = torchvision.transforms.Compose(transforms)\n",
        "testset = datasets.MNIST('./data', train=False, transform=testtransforms)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-yFUhBEgySC",
        "colab_type": "code",
        "outputId": "a6074fc0-634f-45c7-d199-f06a9eece36a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b47rjB-hjTQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    log_interval = 200\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader, datasetname = \"Test\"):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(datasetname,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return test_loss, correct / len(test_loader.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyQBX_nek7Qo",
        "colab_type": "code",
        "outputId": "75c6251f-a425-4904-c128-9df73d757090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Net().to(device)\n",
        "history_train = []\n",
        "history_test = []\n",
        "for lr in [0.1, 0.01, 0.001, 0.0001]:\n",
        "  print(\"Learning rate:\", lr)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
        "\n",
        "  for epoch in range(1, 10 + 1):\n",
        "      train(model, device, trainloader, optimizer, epoch)\n",
        "      history_test.append(test(model, device, test_loader, \"Test\"))\n",
        "      history_train.append(test(model, device, trainloader, \"Train\"))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.1\n",
            "Train Epoch: 1 [0/360000 (0.0%)]\tLoss: 2.298272\n",
            "Train Epoch: 1 [25000/360000 (6.9%)]\tLoss: 0.300703\n",
            "Train Epoch: 1 [50000/360000 (13.9%)]\tLoss: 0.191797\n",
            "Train Epoch: 1 [75000/360000 (20.8%)]\tLoss: 0.097032\n",
            "Train Epoch: 1 [100000/360000 (27.8%)]\tLoss: 0.079423\n",
            "Train Epoch: 1 [125000/360000 (34.7%)]\tLoss: 0.096656\n",
            "Train Epoch: 1 [150000/360000 (41.7%)]\tLoss: 0.161402\n",
            "Train Epoch: 1 [175000/360000 (48.6%)]\tLoss: 0.091698\n",
            "Train Epoch: 1 [200000/360000 (55.6%)]\tLoss: 0.018975\n",
            "Train Epoch: 1 [225000/360000 (62.5%)]\tLoss: 0.091461\n",
            "Train Epoch: 1 [250000/360000 (69.4%)]\tLoss: 0.058985\n",
            "Train Epoch: 1 [275000/360000 (76.4%)]\tLoss: 0.092437\n",
            "Train Epoch: 1 [300000/360000 (83.3%)]\tLoss: 0.099883\n",
            "Train Epoch: 1 [325000/360000 (90.3%)]\tLoss: 0.061512\n",
            "Train Epoch: 1 [350000/360000 (97.2%)]\tLoss: 0.018165\n",
            "\n",
            "Test set: Average loss: 0.0249, Accuracy: 9917/10000 (99.17%)\n",
            "\n",
            "Train set: Average loss: 0.0579, Accuracy: 353527/360000 (98.20%)\n",
            "Train Epoch: 2 [0/360000 (0.0%)]\tLoss: 0.022843\n",
            "Train Epoch: 2 [25000/360000 (6.9%)]\tLoss: 0.071686\n",
            "Train Epoch: 2 [50000/360000 (13.9%)]\tLoss: 0.070285\n",
            "Train Epoch: 2 [75000/360000 (20.8%)]\tLoss: 0.046580\n",
            "Train Epoch: 2 [100000/360000 (27.8%)]\tLoss: 0.106011\n",
            "Train Epoch: 2 [125000/360000 (34.7%)]\tLoss: 0.101357\n",
            "Train Epoch: 2 [150000/360000 (41.7%)]\tLoss: 0.014947\n",
            "Train Epoch: 2 [175000/360000 (48.6%)]\tLoss: 0.040750\n",
            "Train Epoch: 2 [200000/360000 (55.6%)]\tLoss: 0.045388\n",
            "Train Epoch: 2 [225000/360000 (62.5%)]\tLoss: 0.030286\n",
            "Train Epoch: 2 [250000/360000 (69.4%)]\tLoss: 0.050996\n",
            "Train Epoch: 2 [275000/360000 (76.4%)]\tLoss: 0.021220\n",
            "Train Epoch: 2 [300000/360000 (83.3%)]\tLoss: 0.039664\n",
            "Train Epoch: 2 [325000/360000 (90.3%)]\tLoss: 0.028899\n",
            "Train Epoch: 2 [350000/360000 (97.2%)]\tLoss: 0.007331\n",
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "Train set: Average loss: 0.0413, Accuracy: 355273/360000 (98.69%)\n",
            "Train Epoch: 3 [0/360000 (0.0%)]\tLoss: 0.033779\n",
            "Train Epoch: 3 [25000/360000 (6.9%)]\tLoss: 0.011154\n",
            "Train Epoch: 3 [50000/360000 (13.9%)]\tLoss: 0.037368\n",
            "Train Epoch: 3 [75000/360000 (20.8%)]\tLoss: 0.148580\n",
            "Train Epoch: 3 [100000/360000 (27.8%)]\tLoss: 0.026645\n",
            "Train Epoch: 3 [125000/360000 (34.7%)]\tLoss: 0.005522\n",
            "Train Epoch: 3 [150000/360000 (41.7%)]\tLoss: 0.042262\n",
            "Train Epoch: 3 [175000/360000 (48.6%)]\tLoss: 0.045722\n",
            "Train Epoch: 3 [200000/360000 (55.6%)]\tLoss: 0.069693\n",
            "Train Epoch: 3 [225000/360000 (62.5%)]\tLoss: 0.063552\n",
            "Train Epoch: 3 [250000/360000 (69.4%)]\tLoss: 0.007797\n",
            "Train Epoch: 3 [275000/360000 (76.4%)]\tLoss: 0.011713\n",
            "Train Epoch: 3 [300000/360000 (83.3%)]\tLoss: 0.020306\n",
            "Train Epoch: 3 [325000/360000 (90.3%)]\tLoss: 0.095995\n",
            "Train Epoch: 3 [350000/360000 (97.2%)]\tLoss: 0.048803\n",
            "\n",
            "Test set: Average loss: 0.0240, Accuracy: 9923/10000 (99.23%)\n",
            "\n",
            "Train set: Average loss: 0.0425, Accuracy: 355264/360000 (98.68%)\n",
            "Train Epoch: 4 [0/360000 (0.0%)]\tLoss: 0.042945\n",
            "Train Epoch: 4 [25000/360000 (6.9%)]\tLoss: 0.074166\n",
            "Train Epoch: 4 [50000/360000 (13.9%)]\tLoss: 0.004043\n",
            "Train Epoch: 4 [75000/360000 (20.8%)]\tLoss: 0.049496\n",
            "Train Epoch: 4 [100000/360000 (27.8%)]\tLoss: 0.069589\n",
            "Train Epoch: 4 [125000/360000 (34.7%)]\tLoss: 0.113437\n",
            "Train Epoch: 4 [150000/360000 (41.7%)]\tLoss: 0.034935\n",
            "Train Epoch: 4 [175000/360000 (48.6%)]\tLoss: 0.096722\n",
            "Train Epoch: 4 [200000/360000 (55.6%)]\tLoss: 0.037977\n",
            "Train Epoch: 4 [225000/360000 (62.5%)]\tLoss: 0.030470\n",
            "Train Epoch: 4 [250000/360000 (69.4%)]\tLoss: 0.123053\n",
            "Train Epoch: 4 [275000/360000 (76.4%)]\tLoss: 0.027818\n",
            "Train Epoch: 4 [300000/360000 (83.3%)]\tLoss: 0.011506\n",
            "Train Epoch: 4 [325000/360000 (90.3%)]\tLoss: 0.017244\n",
            "Train Epoch: 4 [350000/360000 (97.2%)]\tLoss: 0.031938\n",
            "\n",
            "Test set: Average loss: 0.0214, Accuracy: 9933/10000 (99.33%)\n",
            "\n",
            "Train set: Average loss: 0.0315, Accuracy: 356439/360000 (99.01%)\n",
            "Train Epoch: 5 [0/360000 (0.0%)]\tLoss: 0.020842\n",
            "Train Epoch: 5 [25000/360000 (6.9%)]\tLoss: 0.034363\n",
            "Train Epoch: 5 [50000/360000 (13.9%)]\tLoss: 0.020875\n",
            "Train Epoch: 5 [75000/360000 (20.8%)]\tLoss: 0.098638\n",
            "Train Epoch: 5 [100000/360000 (27.8%)]\tLoss: 0.048938\n",
            "Train Epoch: 5 [125000/360000 (34.7%)]\tLoss: 0.060672\n",
            "Train Epoch: 5 [150000/360000 (41.7%)]\tLoss: 0.025820\n",
            "Train Epoch: 5 [175000/360000 (48.6%)]\tLoss: 0.007777\n",
            "Train Epoch: 5 [200000/360000 (55.6%)]\tLoss: 0.019209\n",
            "Train Epoch: 5 [225000/360000 (62.5%)]\tLoss: 0.043709\n",
            "Train Epoch: 5 [250000/360000 (69.4%)]\tLoss: 0.014152\n",
            "Train Epoch: 5 [275000/360000 (76.4%)]\tLoss: 0.013998\n",
            "Train Epoch: 5 [300000/360000 (83.3%)]\tLoss: 0.018318\n",
            "Train Epoch: 5 [325000/360000 (90.3%)]\tLoss: 0.016888\n",
            "Train Epoch: 5 [350000/360000 (97.2%)]\tLoss: 0.029465\n",
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 9942/10000 (99.42%)\n",
            "\n",
            "Train set: Average loss: 0.0329, Accuracy: 356263/360000 (98.96%)\n",
            "Train Epoch: 6 [0/360000 (0.0%)]\tLoss: 0.052825\n",
            "Train Epoch: 6 [25000/360000 (6.9%)]\tLoss: 0.026035\n",
            "Train Epoch: 6 [50000/360000 (13.9%)]\tLoss: 0.066627\n",
            "Train Epoch: 6 [75000/360000 (20.8%)]\tLoss: 0.007939\n",
            "Train Epoch: 6 [100000/360000 (27.8%)]\tLoss: 0.004996\n",
            "Train Epoch: 6 [125000/360000 (34.7%)]\tLoss: 0.031432\n",
            "Train Epoch: 6 [150000/360000 (41.7%)]\tLoss: 0.031957\n",
            "Train Epoch: 6 [175000/360000 (48.6%)]\tLoss: 0.020436\n",
            "Train Epoch: 6 [200000/360000 (55.6%)]\tLoss: 0.012228\n",
            "Train Epoch: 6 [225000/360000 (62.5%)]\tLoss: 0.092351\n",
            "Train Epoch: 6 [250000/360000 (69.4%)]\tLoss: 0.047666\n",
            "Train Epoch: 6 [275000/360000 (76.4%)]\tLoss: 0.067670\n",
            "Train Epoch: 6 [300000/360000 (83.3%)]\tLoss: 0.033873\n",
            "Train Epoch: 6 [325000/360000 (90.3%)]\tLoss: 0.007713\n",
            "Train Epoch: 6 [350000/360000 (97.2%)]\tLoss: 0.017840\n",
            "\n",
            "Test set: Average loss: 0.0160, Accuracy: 9947/10000 (99.47%)\n",
            "\n",
            "Train set: Average loss: 0.0254, Accuracy: 357138/360000 (99.20%)\n",
            "Train Epoch: 7 [0/360000 (0.0%)]\tLoss: 0.017762\n",
            "Train Epoch: 7 [25000/360000 (6.9%)]\tLoss: 0.050223\n",
            "Train Epoch: 7 [50000/360000 (13.9%)]\tLoss: 0.020166\n",
            "Train Epoch: 7 [75000/360000 (20.8%)]\tLoss: 0.002473\n",
            "Train Epoch: 7 [100000/360000 (27.8%)]\tLoss: 0.024603\n",
            "Train Epoch: 7 [125000/360000 (34.7%)]\tLoss: 0.047208\n",
            "Train Epoch: 7 [150000/360000 (41.7%)]\tLoss: 0.058754\n",
            "Train Epoch: 7 [175000/360000 (48.6%)]\tLoss: 0.002836\n",
            "Train Epoch: 7 [200000/360000 (55.6%)]\tLoss: 0.013452\n",
            "Train Epoch: 7 [225000/360000 (62.5%)]\tLoss: 0.047851\n",
            "Train Epoch: 7 [250000/360000 (69.4%)]\tLoss: 0.002902\n",
            "Train Epoch: 7 [275000/360000 (76.4%)]\tLoss: 0.015353\n",
            "Train Epoch: 7 [300000/360000 (83.3%)]\tLoss: 0.046286\n",
            "Train Epoch: 7 [325000/360000 (90.3%)]\tLoss: 0.026641\n",
            "Train Epoch: 7 [350000/360000 (97.2%)]\tLoss: 0.018847\n",
            "\n",
            "Test set: Average loss: 0.0167, Accuracy: 9949/10000 (99.49%)\n",
            "\n",
            "Train set: Average loss: 0.0236, Accuracy: 357361/360000 (99.27%)\n",
            "Train Epoch: 8 [0/360000 (0.0%)]\tLoss: 0.009942\n",
            "Train Epoch: 8 [25000/360000 (6.9%)]\tLoss: 0.029007\n",
            "Train Epoch: 8 [50000/360000 (13.9%)]\tLoss: 0.071744\n",
            "Train Epoch: 8 [75000/360000 (20.8%)]\tLoss: 0.025433\n",
            "Train Epoch: 8 [100000/360000 (27.8%)]\tLoss: 0.005939\n",
            "Train Epoch: 8 [125000/360000 (34.7%)]\tLoss: 0.003258\n",
            "Train Epoch: 8 [150000/360000 (41.7%)]\tLoss: 0.003317\n",
            "Train Epoch: 8 [175000/360000 (48.6%)]\tLoss: 0.009931\n",
            "Train Epoch: 8 [200000/360000 (55.6%)]\tLoss: 0.002692\n",
            "Train Epoch: 8 [225000/360000 (62.5%)]\tLoss: 0.008117\n",
            "Train Epoch: 8 [250000/360000 (69.4%)]\tLoss: 0.088328\n",
            "Train Epoch: 8 [275000/360000 (76.4%)]\tLoss: 0.003282\n",
            "Train Epoch: 8 [300000/360000 (83.3%)]\tLoss: 0.016101\n",
            "Train Epoch: 8 [325000/360000 (90.3%)]\tLoss: 0.015463\n",
            "Train Epoch: 8 [350000/360000 (97.2%)]\tLoss: 0.018903\n",
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 9946/10000 (99.46%)\n",
            "\n",
            "Train set: Average loss: 0.0244, Accuracy: 357187/360000 (99.22%)\n",
            "Train Epoch: 9 [0/360000 (0.0%)]\tLoss: 0.003502\n",
            "Train Epoch: 9 [25000/360000 (6.9%)]\tLoss: 0.009046\n",
            "Train Epoch: 9 [50000/360000 (13.9%)]\tLoss: 0.004270\n",
            "Train Epoch: 9 [75000/360000 (20.8%)]\tLoss: 0.011028\n",
            "Train Epoch: 9 [100000/360000 (27.8%)]\tLoss: 0.005784\n",
            "Train Epoch: 9 [125000/360000 (34.7%)]\tLoss: 0.003116\n",
            "Train Epoch: 9 [150000/360000 (41.7%)]\tLoss: 0.034256\n",
            "Train Epoch: 9 [175000/360000 (48.6%)]\tLoss: 0.081523\n",
            "Train Epoch: 9 [200000/360000 (55.6%)]\tLoss: 0.019628\n",
            "Train Epoch: 9 [225000/360000 (62.5%)]\tLoss: 0.013142\n",
            "Train Epoch: 9 [250000/360000 (69.4%)]\tLoss: 0.066265\n",
            "Train Epoch: 9 [275000/360000 (76.4%)]\tLoss: 0.001374\n",
            "Train Epoch: 9 [300000/360000 (83.3%)]\tLoss: 0.003825\n",
            "Train Epoch: 9 [325000/360000 (90.3%)]\tLoss: 0.049329\n",
            "Train Epoch: 9 [350000/360000 (97.2%)]\tLoss: 0.068236\n",
            "\n",
            "Test set: Average loss: 0.0196, Accuracy: 9943/10000 (99.43%)\n",
            "\n",
            "Train set: Average loss: 0.0236, Accuracy: 357323/360000 (99.26%)\n",
            "Train Epoch: 10 [0/360000 (0.0%)]\tLoss: 0.088030\n",
            "Train Epoch: 10 [25000/360000 (6.9%)]\tLoss: 0.019128\n",
            "Train Epoch: 10 [50000/360000 (13.9%)]\tLoss: 0.005143\n",
            "Train Epoch: 10 [75000/360000 (20.8%)]\tLoss: 0.076361\n",
            "Train Epoch: 10 [100000/360000 (27.8%)]\tLoss: 0.084460\n",
            "Train Epoch: 10 [125000/360000 (34.7%)]\tLoss: 0.031265\n",
            "Train Epoch: 10 [150000/360000 (41.7%)]\tLoss: 0.019348\n",
            "Train Epoch: 10 [175000/360000 (48.6%)]\tLoss: 0.040275\n",
            "Train Epoch: 10 [200000/360000 (55.6%)]\tLoss: 0.008628\n",
            "Train Epoch: 10 [225000/360000 (62.5%)]\tLoss: 0.029582\n",
            "Train Epoch: 10 [250000/360000 (69.4%)]\tLoss: 0.043594\n",
            "Train Epoch: 10 [275000/360000 (76.4%)]\tLoss: 0.114702\n",
            "Train Epoch: 10 [300000/360000 (83.3%)]\tLoss: 0.026431\n",
            "Train Epoch: 10 [325000/360000 (90.3%)]\tLoss: 0.003428\n",
            "Train Epoch: 10 [350000/360000 (97.2%)]\tLoss: 0.000605\n",
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 9941/10000 (99.41%)\n",
            "\n",
            "Train set: Average loss: 0.0209, Accuracy: 357558/360000 (99.32%)\n",
            "Learning rate: 0.01\n",
            "Train Epoch: 1 [0/360000 (0.0%)]\tLoss: 0.013068\n",
            "Train Epoch: 1 [25000/360000 (6.9%)]\tLoss: 0.006855\n",
            "Train Epoch: 1 [50000/360000 (13.9%)]\tLoss: 0.010137\n",
            "Train Epoch: 1 [75000/360000 (20.8%)]\tLoss: 0.004220\n",
            "Train Epoch: 1 [100000/360000 (27.8%)]\tLoss: 0.000986\n",
            "Train Epoch: 1 [125000/360000 (34.7%)]\tLoss: 0.049755\n",
            "Train Epoch: 1 [150000/360000 (41.7%)]\tLoss: 0.019705\n",
            "Train Epoch: 1 [175000/360000 (48.6%)]\tLoss: 0.000848\n",
            "Train Epoch: 1 [200000/360000 (55.6%)]\tLoss: 0.066609\n",
            "Train Epoch: 1 [225000/360000 (62.5%)]\tLoss: 0.011520\n",
            "Train Epoch: 1 [250000/360000 (69.4%)]\tLoss: 0.004326\n",
            "Train Epoch: 1 [275000/360000 (76.4%)]\tLoss: 0.029743\n",
            "Train Epoch: 1 [300000/360000 (83.3%)]\tLoss: 0.006101\n",
            "Train Epoch: 1 [325000/360000 (90.3%)]\tLoss: 0.021455\n",
            "Train Epoch: 1 [350000/360000 (97.2%)]\tLoss: 0.025844\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 9955/10000 (99.55%)\n",
            "\n",
            "Train set: Average loss: 0.0146, Accuracy: 358364/360000 (99.55%)\n",
            "Train Epoch: 2 [0/360000 (0.0%)]\tLoss: 0.029064\n",
            "Train Epoch: 2 [25000/360000 (6.9%)]\tLoss: 0.025215\n",
            "Train Epoch: 2 [50000/360000 (13.9%)]\tLoss: 0.017882\n",
            "Train Epoch: 2 [75000/360000 (20.8%)]\tLoss: 0.010059\n",
            "Train Epoch: 2 [100000/360000 (27.8%)]\tLoss: 0.005711\n",
            "Train Epoch: 2 [125000/360000 (34.7%)]\tLoss: 0.017124\n",
            "Train Epoch: 2 [150000/360000 (41.7%)]\tLoss: 0.006779\n",
            "Train Epoch: 2 [175000/360000 (48.6%)]\tLoss: 0.003338\n",
            "Train Epoch: 2 [200000/360000 (55.6%)]\tLoss: 0.035375\n",
            "Train Epoch: 2 [225000/360000 (62.5%)]\tLoss: 0.060164\n",
            "Train Epoch: 2 [250000/360000 (69.4%)]\tLoss: 0.017827\n",
            "Train Epoch: 2 [275000/360000 (76.4%)]\tLoss: 0.039666\n",
            "Train Epoch: 2 [300000/360000 (83.3%)]\tLoss: 0.006821\n",
            "Train Epoch: 2 [325000/360000 (90.3%)]\tLoss: 0.027580\n",
            "Train Epoch: 2 [350000/360000 (97.2%)]\tLoss: 0.007236\n",
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 9956/10000 (99.56%)\n",
            "\n",
            "Train set: Average loss: 0.0137, Accuracy: 358431/360000 (99.56%)\n",
            "Train Epoch: 3 [0/360000 (0.0%)]\tLoss: 0.000965\n",
            "Train Epoch: 3 [25000/360000 (6.9%)]\tLoss: 0.001900\n",
            "Train Epoch: 3 [50000/360000 (13.9%)]\tLoss: 0.018766\n",
            "Train Epoch: 3 [75000/360000 (20.8%)]\tLoss: 0.002407\n",
            "Train Epoch: 3 [100000/360000 (27.8%)]\tLoss: 0.001381\n",
            "Train Epoch: 3 [125000/360000 (34.7%)]\tLoss: 0.058846\n",
            "Train Epoch: 3 [150000/360000 (41.7%)]\tLoss: 0.028349\n",
            "Train Epoch: 3 [175000/360000 (48.6%)]\tLoss: 0.037087\n",
            "Train Epoch: 3 [200000/360000 (55.6%)]\tLoss: 0.009489\n",
            "Train Epoch: 3 [225000/360000 (62.5%)]\tLoss: 0.003249\n",
            "Train Epoch: 3 [250000/360000 (69.4%)]\tLoss: 0.005637\n",
            "Train Epoch: 3 [275000/360000 (76.4%)]\tLoss: 0.007661\n",
            "Train Epoch: 3 [300000/360000 (83.3%)]\tLoss: 0.013263\n",
            "Train Epoch: 3 [325000/360000 (90.3%)]\tLoss: 0.001388\n",
            "Train Epoch: 3 [350000/360000 (97.2%)]\tLoss: 0.001967\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 9953/10000 (99.53%)\n",
            "\n",
            "Train set: Average loss: 0.0131, Accuracy: 358522/360000 (99.59%)\n",
            "Train Epoch: 4 [0/360000 (0.0%)]\tLoss: 0.001007\n",
            "Train Epoch: 4 [25000/360000 (6.9%)]\tLoss: 0.002587\n",
            "Train Epoch: 4 [50000/360000 (13.9%)]\tLoss: 0.040305\n",
            "Train Epoch: 4 [75000/360000 (20.8%)]\tLoss: 0.015962\n",
            "Train Epoch: 4 [100000/360000 (27.8%)]\tLoss: 0.036504\n",
            "Train Epoch: 4 [125000/360000 (34.7%)]\tLoss: 0.004894\n",
            "Train Epoch: 4 [150000/360000 (41.7%)]\tLoss: 0.009350\n",
            "Train Epoch: 4 [175000/360000 (48.6%)]\tLoss: 0.034843\n",
            "Train Epoch: 4 [200000/360000 (55.6%)]\tLoss: 0.000269\n",
            "Train Epoch: 4 [225000/360000 (62.5%)]\tLoss: 0.013592\n",
            "Train Epoch: 4 [250000/360000 (69.4%)]\tLoss: 0.034815\n",
            "Train Epoch: 4 [275000/360000 (76.4%)]\tLoss: 0.022456\n",
            "Train Epoch: 4 [300000/360000 (83.3%)]\tLoss: 0.029567\n",
            "Train Epoch: 4 [325000/360000 (90.3%)]\tLoss: 0.001486\n",
            "Train Epoch: 4 [350000/360000 (97.2%)]\tLoss: 0.000320\n",
            "\n",
            "Test set: Average loss: 0.0137, Accuracy: 9960/10000 (99.60%)\n",
            "\n",
            "Train set: Average loss: 0.0129, Accuracy: 358488/360000 (99.58%)\n",
            "Train Epoch: 5 [0/360000 (0.0%)]\tLoss: 0.013071\n",
            "Train Epoch: 5 [25000/360000 (6.9%)]\tLoss: 0.007106\n",
            "Train Epoch: 5 [50000/360000 (13.9%)]\tLoss: 0.005497\n",
            "Train Epoch: 5 [75000/360000 (20.8%)]\tLoss: 0.012560\n",
            "Train Epoch: 5 [100000/360000 (27.8%)]\tLoss: 0.018423\n",
            "Train Epoch: 5 [125000/360000 (34.7%)]\tLoss: 0.021328\n",
            "Train Epoch: 5 [150000/360000 (41.7%)]\tLoss: 0.000993\n",
            "Train Epoch: 5 [175000/360000 (48.6%)]\tLoss: 0.003523\n",
            "Train Epoch: 5 [200000/360000 (55.6%)]\tLoss: 0.029303\n",
            "Train Epoch: 5 [225000/360000 (62.5%)]\tLoss: 0.016173\n",
            "Train Epoch: 5 [250000/360000 (69.4%)]\tLoss: 0.037783\n",
            "Train Epoch: 5 [275000/360000 (76.4%)]\tLoss: 0.012551\n",
            "Train Epoch: 5 [300000/360000 (83.3%)]\tLoss: 0.002256\n",
            "Train Epoch: 5 [325000/360000 (90.3%)]\tLoss: 0.002507\n",
            "Train Epoch: 5 [350000/360000 (97.2%)]\tLoss: 0.007611\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 9955/10000 (99.55%)\n",
            "\n",
            "Train set: Average loss: 0.0128, Accuracy: 358531/360000 (99.59%)\n",
            "Train Epoch: 6 [0/360000 (0.0%)]\tLoss: 0.003250\n",
            "Train Epoch: 6 [25000/360000 (6.9%)]\tLoss: 0.009917\n",
            "Train Epoch: 6 [50000/360000 (13.9%)]\tLoss: 0.039201\n",
            "Train Epoch: 6 [75000/360000 (20.8%)]\tLoss: 0.003593\n",
            "Train Epoch: 6 [100000/360000 (27.8%)]\tLoss: 0.005624\n",
            "Train Epoch: 6 [125000/360000 (34.7%)]\tLoss: 0.001909\n",
            "Train Epoch: 6 [150000/360000 (41.7%)]\tLoss: 0.001299\n",
            "Train Epoch: 6 [175000/360000 (48.6%)]\tLoss: 0.015607\n",
            "Train Epoch: 6 [200000/360000 (55.6%)]\tLoss: 0.001555\n",
            "Train Epoch: 6 [225000/360000 (62.5%)]\tLoss: 0.003635\n",
            "Train Epoch: 6 [250000/360000 (69.4%)]\tLoss: 0.046789\n",
            "Train Epoch: 6 [275000/360000 (76.4%)]\tLoss: 0.013814\n",
            "Train Epoch: 6 [300000/360000 (83.3%)]\tLoss: 0.001073\n",
            "Train Epoch: 6 [325000/360000 (90.3%)]\tLoss: 0.020146\n",
            "Train Epoch: 6 [350000/360000 (97.2%)]\tLoss: 0.002751\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 9956/10000 (99.56%)\n",
            "\n",
            "Train set: Average loss: 0.0128, Accuracy: 358550/360000 (99.60%)\n",
            "Train Epoch: 7 [0/360000 (0.0%)]\tLoss: 0.002685\n",
            "Train Epoch: 7 [25000/360000 (6.9%)]\tLoss: 0.007116\n",
            "Train Epoch: 7 [50000/360000 (13.9%)]\tLoss: 0.006502\n",
            "Train Epoch: 7 [75000/360000 (20.8%)]\tLoss: 0.008183\n",
            "Train Epoch: 7 [100000/360000 (27.8%)]\tLoss: 0.007953\n",
            "Train Epoch: 7 [125000/360000 (34.7%)]\tLoss: 0.005082\n",
            "Train Epoch: 7 [150000/360000 (41.7%)]\tLoss: 0.002618\n",
            "Train Epoch: 7 [175000/360000 (48.6%)]\tLoss: 0.002002\n",
            "Train Epoch: 7 [200000/360000 (55.6%)]\tLoss: 0.001958\n",
            "Train Epoch: 7 [225000/360000 (62.5%)]\tLoss: 0.001108\n",
            "Train Epoch: 7 [250000/360000 (69.4%)]\tLoss: 0.032699\n",
            "Train Epoch: 7 [275000/360000 (76.4%)]\tLoss: 0.011877\n",
            "Train Epoch: 7 [300000/360000 (83.3%)]\tLoss: 0.011424\n",
            "Train Epoch: 7 [325000/360000 (90.3%)]\tLoss: 0.011451\n",
            "Train Epoch: 7 [350000/360000 (97.2%)]\tLoss: 0.000671\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 9953/10000 (99.53%)\n",
            "\n",
            "Train set: Average loss: 0.0122, Accuracy: 358614/360000 (99.61%)\n",
            "Train Epoch: 8 [0/360000 (0.0%)]\tLoss: 0.001412\n",
            "Train Epoch: 8 [25000/360000 (6.9%)]\tLoss: 0.048016\n",
            "Train Epoch: 8 [50000/360000 (13.9%)]\tLoss: 0.001755\n",
            "Train Epoch: 8 [75000/360000 (20.8%)]\tLoss: 0.003192\n",
            "Train Epoch: 8 [100000/360000 (27.8%)]\tLoss: 0.006225\n",
            "Train Epoch: 8 [125000/360000 (34.7%)]\tLoss: 0.000609\n",
            "Train Epoch: 8 [150000/360000 (41.7%)]\tLoss: 0.001984\n",
            "Train Epoch: 8 [175000/360000 (48.6%)]\tLoss: 0.059018\n",
            "Train Epoch: 8 [200000/360000 (55.6%)]\tLoss: 0.004919\n",
            "Train Epoch: 8 [225000/360000 (62.5%)]\tLoss: 0.004862\n",
            "Train Epoch: 8 [250000/360000 (69.4%)]\tLoss: 0.003374\n",
            "Train Epoch: 8 [275000/360000 (76.4%)]\tLoss: 0.004567\n",
            "Train Epoch: 8 [300000/360000 (83.3%)]\tLoss: 0.000533\n",
            "Train Epoch: 8 [325000/360000 (90.3%)]\tLoss: 0.075631\n",
            "Train Epoch: 8 [350000/360000 (97.2%)]\tLoss: 0.000622\n",
            "\n",
            "Test set: Average loss: 0.0137, Accuracy: 9954/10000 (99.54%)\n",
            "\n",
            "Train set: Average loss: 0.0120, Accuracy: 358633/360000 (99.62%)\n",
            "Train Epoch: 9 [0/360000 (0.0%)]\tLoss: 0.017465\n",
            "Train Epoch: 9 [25000/360000 (6.9%)]\tLoss: 0.025809\n",
            "Train Epoch: 9 [50000/360000 (13.9%)]\tLoss: 0.002265\n",
            "Train Epoch: 9 [75000/360000 (20.8%)]\tLoss: 0.002085\n",
            "Train Epoch: 9 [100000/360000 (27.8%)]\tLoss: 0.015256\n",
            "Train Epoch: 9 [125000/360000 (34.7%)]\tLoss: 0.000920\n",
            "Train Epoch: 9 [150000/360000 (41.7%)]\tLoss: 0.012814\n",
            "Train Epoch: 9 [175000/360000 (48.6%)]\tLoss: 0.000807\n",
            "Train Epoch: 9 [200000/360000 (55.6%)]\tLoss: 0.022219\n",
            "Train Epoch: 9 [225000/360000 (62.5%)]\tLoss: 0.001159\n",
            "Train Epoch: 9 [250000/360000 (69.4%)]\tLoss: 0.013692\n",
            "Train Epoch: 9 [275000/360000 (76.4%)]\tLoss: 0.016259\n",
            "Train Epoch: 9 [300000/360000 (83.3%)]\tLoss: 0.003048\n",
            "Train Epoch: 9 [325000/360000 (90.3%)]\tLoss: 0.042049\n",
            "Train Epoch: 9 [350000/360000 (97.2%)]\tLoss: 0.006153\n",
            "\n",
            "Test set: Average loss: 0.0137, Accuracy: 9958/10000 (99.58%)\n",
            "\n",
            "Train set: Average loss: 0.0117, Accuracy: 358653/360000 (99.63%)\n",
            "Train Epoch: 10 [0/360000 (0.0%)]\tLoss: 0.003215\n",
            "Train Epoch: 10 [25000/360000 (6.9%)]\tLoss: 0.001349\n",
            "Train Epoch: 10 [50000/360000 (13.9%)]\tLoss: 0.012479\n",
            "Train Epoch: 10 [75000/360000 (20.8%)]\tLoss: 0.000635\n",
            "Train Epoch: 10 [100000/360000 (27.8%)]\tLoss: 0.022651\n",
            "Train Epoch: 10 [125000/360000 (34.7%)]\tLoss: 0.050231\n",
            "Train Epoch: 10 [150000/360000 (41.7%)]\tLoss: 0.002631\n",
            "Train Epoch: 10 [175000/360000 (48.6%)]\tLoss: 0.014704\n",
            "Train Epoch: 10 [200000/360000 (55.6%)]\tLoss: 0.016845\n",
            "Train Epoch: 10 [225000/360000 (62.5%)]\tLoss: 0.001870\n",
            "Train Epoch: 10 [250000/360000 (69.4%)]\tLoss: 0.012642\n",
            "Train Epoch: 10 [275000/360000 (76.4%)]\tLoss: 0.032129\n",
            "Train Epoch: 10 [300000/360000 (83.3%)]\tLoss: 0.003741\n",
            "Train Epoch: 10 [325000/360000 (90.3%)]\tLoss: 0.000397\n",
            "Train Epoch: 10 [350000/360000 (97.2%)]\tLoss: 0.000790\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9959/10000 (99.59%)\n",
            "\n",
            "Train set: Average loss: 0.0117, Accuracy: 358641/360000 (99.62%)\n",
            "Learning rate: 0.001\n",
            "Train Epoch: 1 [0/360000 (0.0%)]\tLoss: 0.032641\n",
            "Train Epoch: 1 [25000/360000 (6.9%)]\tLoss: 0.007934\n",
            "Train Epoch: 1 [50000/360000 (13.9%)]\tLoss: 0.002132\n",
            "Train Epoch: 1 [75000/360000 (20.8%)]\tLoss: 0.012035\n",
            "Train Epoch: 1 [100000/360000 (27.8%)]\tLoss: 0.011810\n",
            "Train Epoch: 1 [125000/360000 (34.7%)]\tLoss: 0.051143\n",
            "Train Epoch: 1 [150000/360000 (41.7%)]\tLoss: 0.002961\n",
            "Train Epoch: 1 [175000/360000 (48.6%)]\tLoss: 0.007566\n",
            "Train Epoch: 1 [200000/360000 (55.6%)]\tLoss: 0.027546\n",
            "Train Epoch: 1 [225000/360000 (62.5%)]\tLoss: 0.001566\n",
            "Train Epoch: 1 [250000/360000 (69.4%)]\tLoss: 0.001371\n",
            "Train Epoch: 1 [275000/360000 (76.4%)]\tLoss: 0.000548\n",
            "Train Epoch: 1 [300000/360000 (83.3%)]\tLoss: 0.015712\n",
            "Train Epoch: 1 [325000/360000 (90.3%)]\tLoss: 0.000735\n",
            "Train Epoch: 1 [350000/360000 (97.2%)]\tLoss: 0.019611\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9955/10000 (99.55%)\n",
            "\n",
            "Train set: Average loss: 0.0117, Accuracy: 358659/360000 (99.63%)\n",
            "Train Epoch: 2 [0/360000 (0.0%)]\tLoss: 0.001303\n",
            "Train Epoch: 2 [25000/360000 (6.9%)]\tLoss: 0.001106\n",
            "Train Epoch: 2 [50000/360000 (13.9%)]\tLoss: 0.046239\n",
            "Train Epoch: 2 [75000/360000 (20.8%)]\tLoss: 0.004994\n",
            "Train Epoch: 2 [100000/360000 (27.8%)]\tLoss: 0.001517\n",
            "Train Epoch: 2 [125000/360000 (34.7%)]\tLoss: 0.015527\n",
            "Train Epoch: 2 [150000/360000 (41.7%)]\tLoss: 0.009331\n",
            "Train Epoch: 2 [175000/360000 (48.6%)]\tLoss: 0.004188\n",
            "Train Epoch: 2 [200000/360000 (55.6%)]\tLoss: 0.031681\n",
            "Train Epoch: 2 [225000/360000 (62.5%)]\tLoss: 0.010630\n",
            "Train Epoch: 2 [250000/360000 (69.4%)]\tLoss: 0.007236\n",
            "Train Epoch: 2 [275000/360000 (76.4%)]\tLoss: 0.013813\n",
            "Train Epoch: 2 [300000/360000 (83.3%)]\tLoss: 0.000687\n",
            "Train Epoch: 2 [325000/360000 (90.3%)]\tLoss: 0.006179\n",
            "Train Epoch: 2 [350000/360000 (97.2%)]\tLoss: 0.001236\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9957/10000 (99.57%)\n",
            "\n",
            "Train set: Average loss: 0.0110, Accuracy: 358754/360000 (99.65%)\n",
            "Train Epoch: 3 [0/360000 (0.0%)]\tLoss: 0.022082\n",
            "Train Epoch: 3 [25000/360000 (6.9%)]\tLoss: 0.016030\n",
            "Train Epoch: 3 [50000/360000 (13.9%)]\tLoss: 0.012870\n",
            "Train Epoch: 3 [75000/360000 (20.8%)]\tLoss: 0.037502\n",
            "Train Epoch: 3 [100000/360000 (27.8%)]\tLoss: 0.002196\n",
            "Train Epoch: 3 [125000/360000 (34.7%)]\tLoss: 0.002519\n",
            "Train Epoch: 3 [150000/360000 (41.7%)]\tLoss: 0.008581\n",
            "Train Epoch: 3 [175000/360000 (48.6%)]\tLoss: 0.002516\n",
            "Train Epoch: 3 [200000/360000 (55.6%)]\tLoss: 0.001527\n",
            "Train Epoch: 3 [225000/360000 (62.5%)]\tLoss: 0.008535\n",
            "Train Epoch: 3 [250000/360000 (69.4%)]\tLoss: 0.012115\n",
            "Train Epoch: 3 [275000/360000 (76.4%)]\tLoss: 0.000699\n",
            "Train Epoch: 3 [300000/360000 (83.3%)]\tLoss: 0.002561\n",
            "Train Epoch: 3 [325000/360000 (90.3%)]\tLoss: 0.005058\n",
            "Train Epoch: 3 [350000/360000 (97.2%)]\tLoss: 0.000673\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9954/10000 (99.54%)\n",
            "\n",
            "Train set: Average loss: 0.0116, Accuracy: 358697/360000 (99.64%)\n",
            "Train Epoch: 4 [0/360000 (0.0%)]\tLoss: 0.011331\n",
            "Train Epoch: 4 [25000/360000 (6.9%)]\tLoss: 0.000315\n",
            "Train Epoch: 4 [50000/360000 (13.9%)]\tLoss: 0.000524\n",
            "Train Epoch: 4 [75000/360000 (20.8%)]\tLoss: 0.030953\n",
            "Train Epoch: 4 [100000/360000 (27.8%)]\tLoss: 0.004464\n",
            "Train Epoch: 4 [125000/360000 (34.7%)]\tLoss: 0.019376\n",
            "Train Epoch: 4 [150000/360000 (41.7%)]\tLoss: 0.001974\n",
            "Train Epoch: 4 [175000/360000 (48.6%)]\tLoss: 0.001494\n",
            "Train Epoch: 4 [200000/360000 (55.6%)]\tLoss: 0.015721\n",
            "Train Epoch: 4 [225000/360000 (62.5%)]\tLoss: 0.011987\n",
            "Train Epoch: 4 [250000/360000 (69.4%)]\tLoss: 0.003655\n",
            "Train Epoch: 4 [275000/360000 (76.4%)]\tLoss: 0.000359\n",
            "Train Epoch: 4 [300000/360000 (83.3%)]\tLoss: 0.006472\n",
            "Train Epoch: 4 [325000/360000 (90.3%)]\tLoss: 0.001194\n",
            "Train Epoch: 4 [350000/360000 (97.2%)]\tLoss: 0.000796\n",
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 9956/10000 (99.56%)\n",
            "\n",
            "Train set: Average loss: 0.0112, Accuracy: 358706/360000 (99.64%)\n",
            "Train Epoch: 5 [0/360000 (0.0%)]\tLoss: 0.001940\n",
            "Train Epoch: 5 [25000/360000 (6.9%)]\tLoss: 0.007381\n",
            "Train Epoch: 5 [50000/360000 (13.9%)]\tLoss: 0.015219\n",
            "Train Epoch: 5 [75000/360000 (20.8%)]\tLoss: 0.000864\n",
            "Train Epoch: 5 [100000/360000 (27.8%)]\tLoss: 0.019470\n",
            "Train Epoch: 5 [125000/360000 (34.7%)]\tLoss: 0.009173\n",
            "Train Epoch: 5 [150000/360000 (41.7%)]\tLoss: 0.010464\n",
            "Train Epoch: 5 [175000/360000 (48.6%)]\tLoss: 0.001636\n",
            "Train Epoch: 5 [200000/360000 (55.6%)]\tLoss: 0.002963\n",
            "Train Epoch: 5 [225000/360000 (62.5%)]\tLoss: 0.004041\n",
            "Train Epoch: 5 [250000/360000 (69.4%)]\tLoss: 0.010534\n",
            "Train Epoch: 5 [275000/360000 (76.4%)]\tLoss: 0.004718\n",
            "Train Epoch: 5 [300000/360000 (83.3%)]\tLoss: 0.000468\n",
            "Train Epoch: 5 [325000/360000 (90.3%)]\tLoss: 0.001536\n",
            "Train Epoch: 5 [350000/360000 (97.2%)]\tLoss: 0.008968\n",
            "\n",
            "Test set: Average loss: 0.0132, Accuracy: 9958/10000 (99.58%)\n",
            "\n",
            "Train set: Average loss: 0.0115, Accuracy: 358682/360000 (99.63%)\n",
            "Train Epoch: 6 [0/360000 (0.0%)]\tLoss: 0.015560\n",
            "Train Epoch: 6 [25000/360000 (6.9%)]\tLoss: 0.006058\n",
            "Train Epoch: 6 [50000/360000 (13.9%)]\tLoss: 0.001460\n",
            "Train Epoch: 6 [75000/360000 (20.8%)]\tLoss: 0.004313\n",
            "Train Epoch: 6 [100000/360000 (27.8%)]\tLoss: 0.016617\n",
            "Train Epoch: 6 [125000/360000 (34.7%)]\tLoss: 0.007101\n",
            "Train Epoch: 6 [150000/360000 (41.7%)]\tLoss: 0.005675\n",
            "Train Epoch: 6 [175000/360000 (48.6%)]\tLoss: 0.000410\n",
            "Train Epoch: 6 [200000/360000 (55.6%)]\tLoss: 0.008399\n",
            "Train Epoch: 6 [225000/360000 (62.5%)]\tLoss: 0.000922\n",
            "Train Epoch: 6 [250000/360000 (69.4%)]\tLoss: 0.003977\n",
            "Train Epoch: 6 [275000/360000 (76.4%)]\tLoss: 0.024064\n",
            "Train Epoch: 6 [300000/360000 (83.3%)]\tLoss: 0.000993\n",
            "Train Epoch: 6 [325000/360000 (90.3%)]\tLoss: 0.001514\n",
            "Train Epoch: 6 [350000/360000 (97.2%)]\tLoss: 0.000302\n",
            "\n",
            "Test set: Average loss: 0.0133, Accuracy: 9959/10000 (99.59%)\n",
            "\n",
            "Train set: Average loss: 0.0117, Accuracy: 358678/360000 (99.63%)\n",
            "Train Epoch: 7 [0/360000 (0.0%)]\tLoss: 0.002310\n",
            "Train Epoch: 7 [25000/360000 (6.9%)]\tLoss: 0.003056\n",
            "Train Epoch: 7 [50000/360000 (13.9%)]\tLoss: 0.001019\n",
            "Train Epoch: 7 [75000/360000 (20.8%)]\tLoss: 0.000341\n",
            "Train Epoch: 7 [100000/360000 (27.8%)]\tLoss: 0.001934\n",
            "Train Epoch: 7 [125000/360000 (34.7%)]\tLoss: 0.002480\n",
            "Train Epoch: 7 [150000/360000 (41.7%)]\tLoss: 0.006406\n",
            "Train Epoch: 7 [175000/360000 (48.6%)]\tLoss: 0.001625\n",
            "Train Epoch: 7 [200000/360000 (55.6%)]\tLoss: 0.004125\n",
            "Train Epoch: 7 [225000/360000 (62.5%)]\tLoss: 0.002527\n",
            "Train Epoch: 7 [250000/360000 (69.4%)]\tLoss: 0.036392\n",
            "Train Epoch: 7 [275000/360000 (76.4%)]\tLoss: 0.010444\n",
            "Train Epoch: 7 [300000/360000 (83.3%)]\tLoss: 0.054121\n",
            "Train Epoch: 7 [325000/360000 (90.3%)]\tLoss: 0.002066\n",
            "Train Epoch: 7 [350000/360000 (97.2%)]\tLoss: 0.000487\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9955/10000 (99.55%)\n",
            "\n",
            "Train set: Average loss: 0.0111, Accuracy: 358706/360000 (99.64%)\n",
            "Train Epoch: 8 [0/360000 (0.0%)]\tLoss: 0.015072\n",
            "Train Epoch: 8 [25000/360000 (6.9%)]\tLoss: 0.012951\n",
            "Train Epoch: 8 [50000/360000 (13.9%)]\tLoss: 0.001854\n",
            "Train Epoch: 8 [75000/360000 (20.8%)]\tLoss: 0.002222\n",
            "Train Epoch: 8 [100000/360000 (27.8%)]\tLoss: 0.043590\n",
            "Train Epoch: 8 [125000/360000 (34.7%)]\tLoss: 0.006760\n",
            "Train Epoch: 8 [150000/360000 (41.7%)]\tLoss: 0.001411\n",
            "Train Epoch: 8 [175000/360000 (48.6%)]\tLoss: 0.001210\n",
            "Train Epoch: 8 [200000/360000 (55.6%)]\tLoss: 0.041367\n",
            "Train Epoch: 8 [225000/360000 (62.5%)]\tLoss: 0.014154\n",
            "Train Epoch: 8 [250000/360000 (69.4%)]\tLoss: 0.000967\n",
            "Train Epoch: 8 [275000/360000 (76.4%)]\tLoss: 0.001828\n",
            "Train Epoch: 8 [300000/360000 (83.3%)]\tLoss: 0.005715\n",
            "Train Epoch: 8 [325000/360000 (90.3%)]\tLoss: 0.002984\n",
            "Train Epoch: 8 [350000/360000 (97.2%)]\tLoss: 0.000749\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9956/10000 (99.56%)\n",
            "\n",
            "Train set: Average loss: 0.0112, Accuracy: 358701/360000 (99.64%)\n",
            "Train Epoch: 9 [0/360000 (0.0%)]\tLoss: 0.007161\n",
            "Train Epoch: 9 [25000/360000 (6.9%)]\tLoss: 0.002292\n",
            "Train Epoch: 9 [50000/360000 (13.9%)]\tLoss: 0.008861\n",
            "Train Epoch: 9 [75000/360000 (20.8%)]\tLoss: 0.003100\n",
            "Train Epoch: 9 [100000/360000 (27.8%)]\tLoss: 0.002176\n",
            "Train Epoch: 9 [125000/360000 (34.7%)]\tLoss: 0.000503\n",
            "Train Epoch: 9 [150000/360000 (41.7%)]\tLoss: 0.017741\n",
            "Train Epoch: 9 [175000/360000 (48.6%)]\tLoss: 0.005024\n",
            "Train Epoch: 9 [200000/360000 (55.6%)]\tLoss: 0.001209\n",
            "Train Epoch: 9 [225000/360000 (62.5%)]\tLoss: 0.022093\n",
            "Train Epoch: 9 [250000/360000 (69.4%)]\tLoss: 0.003897\n",
            "Train Epoch: 9 [275000/360000 (76.4%)]\tLoss: 0.025689\n",
            "Train Epoch: 9 [300000/360000 (83.3%)]\tLoss: 0.004988\n",
            "Train Epoch: 9 [325000/360000 (90.3%)]\tLoss: 0.000165\n",
            "Train Epoch: 9 [350000/360000 (97.2%)]\tLoss: 0.001668\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9957/10000 (99.57%)\n",
            "\n",
            "Train set: Average loss: 0.0111, Accuracy: 358740/360000 (99.65%)\n",
            "Train Epoch: 10 [0/360000 (0.0%)]\tLoss: 0.030353\n",
            "Train Epoch: 10 [25000/360000 (6.9%)]\tLoss: 0.018849\n",
            "Train Epoch: 10 [50000/360000 (13.9%)]\tLoss: 0.003876\n",
            "Train Epoch: 10 [75000/360000 (20.8%)]\tLoss: 0.001515\n",
            "Train Epoch: 10 [100000/360000 (27.8%)]\tLoss: 0.000891\n",
            "Train Epoch: 10 [125000/360000 (34.7%)]\tLoss: 0.000576\n",
            "Train Epoch: 10 [150000/360000 (41.7%)]\tLoss: 0.011883\n",
            "Train Epoch: 10 [175000/360000 (48.6%)]\tLoss: 0.024100\n",
            "Train Epoch: 10 [200000/360000 (55.6%)]\tLoss: 0.003248\n",
            "Train Epoch: 10 [225000/360000 (62.5%)]\tLoss: 0.000822\n",
            "Train Epoch: 10 [250000/360000 (69.4%)]\tLoss: 0.046509\n",
            "Train Epoch: 10 [275000/360000 (76.4%)]\tLoss: 0.001750\n",
            "Train Epoch: 10 [300000/360000 (83.3%)]\tLoss: 0.007068\n",
            "Train Epoch: 10 [325000/360000 (90.3%)]\tLoss: 0.008465\n",
            "Train Epoch: 10 [350000/360000 (97.2%)]\tLoss: 0.000629\n",
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 9957/10000 (99.57%)\n",
            "\n",
            "Train set: Average loss: 0.0109, Accuracy: 358744/360000 (99.65%)\n",
            "Learning rate: 0.0001\n",
            "Train Epoch: 1 [0/360000 (0.0%)]\tLoss: 0.001791\n",
            "Train Epoch: 1 [25000/360000 (6.9%)]\tLoss: 0.045981\n",
            "Train Epoch: 1 [50000/360000 (13.9%)]\tLoss: 0.005231\n",
            "Train Epoch: 1 [75000/360000 (20.8%)]\tLoss: 0.003893\n",
            "Train Epoch: 1 [100000/360000 (27.8%)]\tLoss: 0.000975\n",
            "Train Epoch: 1 [125000/360000 (34.7%)]\tLoss: 0.015962\n",
            "Train Epoch: 1 [150000/360000 (41.7%)]\tLoss: 0.018626\n",
            "Train Epoch: 1 [175000/360000 (48.6%)]\tLoss: 0.007723\n",
            "Train Epoch: 1 [200000/360000 (55.6%)]\tLoss: 0.007058\n",
            "Train Epoch: 1 [225000/360000 (62.5%)]\tLoss: 0.012268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-feab35037efa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mhistory_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mhistory_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-eb324fb83921>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJldRWMKlGhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(),\"./mnist_cnn2.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13grBwe-FmX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "df311a24-6420-41a8-f542-5b95e5dbe8e2"
      },
      "source": [
        "def plotprog(title, traindata, testdata):\n",
        "  l1, = plt.plot(range(len(traindata)), traindata, label=\"Training\")\n",
        "  l2, = plt.plot(range(len(testdata)), testdata, label=\"Test\")\n",
        "  plt.legend(handles=[l1,l2])\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "  \n",
        "plotprog(\"Accuracy\", [t[1]*100 for t in history_train], [t[1]*100 for t in history_test])\n",
        "plotprog(\"Loss\", [t[0] for t in history_train], [t[0] for t in history_test])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMSbKWOI3xpgoY4nfGGOizP8HCq7Kx4dBamQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNuBZdthoCK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def see(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            matches = pred.eq(target.view_as(pred)).cpu().numpy().flatten()\n",
        "            pred = pred.cpu().numpy().flatten()[matches == 0]\n",
        "            target = target.cpu().numpy().flatten()[matches == 0]\n",
        "            print(\"pred\", pred.shape)\n",
        "            print(\"target\", target.shape)\n",
        "            print(\"matches\", matches.shape, (matches == 0).shape)\n",
        "            imgs = data.cpu().numpy()[:, 0, :, :][matches == 0]\n",
        "            print(\"mismatches\", (matches == 0).sum())\n",
        "            for i, img in enumerate(imgs):\n",
        "              print(i, \"prediction\", pred[i], \"target\", target[i], output[i].cpu().numpy())\n",
        "              plt.imshow(img)\n",
        "              plt.show()\n",
        "            #break\n",
        "            \n",
        "            #test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            #pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            #correct += pred.eq(target.view_as(pred)).sum().item()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0cXgQR0opaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(model, device, test_loader)\n",
        "see(model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZuGu_G6xLuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ZRkXYXyBdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}